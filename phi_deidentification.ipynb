{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bef590e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7d0ec4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /Users/manibatra/mambaforge/envs/phi/lib/python3.11/site-packages (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/manibatra/mambaforge/envs/phi/lib/python3.11/site-packages (from accelerate) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/manibatra/mambaforge/envs/phi/lib/python3.11/site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in /Users/manibatra/mambaforge/envs/phi/lib/python3.11/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /Users/manibatra/mambaforge/envs/phi/lib/python3.11/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /Users/manibatra/mambaforge/envs/phi/lib/python3.11/site-packages (from accelerate) (2.0.0)\n",
      "Requirement already satisfied: filelock in /Users/manibatra/mambaforge/envs/phi/lib/python3.11/site-packages (from torch>=1.6.0->accelerate) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/manibatra/mambaforge/envs/phi/lib/python3.11/site-packages (from torch>=1.6.0->accelerate) (4.6.2)\n",
      "Requirement already satisfied: sympy in /Users/manibatra/mambaforge/envs/phi/lib/python3.11/site-packages (from torch>=1.6.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/manibatra/mambaforge/envs/phi/lib/python3.11/site-packages (from torch>=1.6.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/manibatra/mambaforge/envs/phi/lib/python3.11/site-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/manibatra/mambaforge/envs/phi/lib/python3.11/site-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/manibatra/mambaforge/envs/phi/lib/python3.11/site-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5591e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_1 = './i2b2_parsing/parsed_gold_set_1.jsonl'\n",
    "test_path = './i2b2_parsing/parsed_test.jsonl'\n",
    "data = pd.read_json(file_path_1, lines=True)\n",
    "test_data = pd.read_json(test_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25780753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spans</th>\n",
       "      <th>meta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\n\\nRecord date: 2080-11-30\\n\\n\\n\\nReason fo...</td>\n",
       "      <td>[{'id': 'P0', 'start': '16', 'end': '26', 'tex...</td>\n",
       "      <td>{'note_id': '369-03', 'patient': '369', 'recor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\nRecord date: 2061-01-15\\n\\n             ...</td>\n",
       "      <td>[{'id': 'P0', 'start': '16', 'end': '26', 'tex...</td>\n",
       "      <td>{'note_id': '299-01', 'patient': '299', 'recor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n\\nRecord date: 2083-04-03\\n\\n\\nBeaumont Ho...</td>\n",
       "      <td>[{'id': 'P0', 'start': '16', 'end': '26', 'tex...</td>\n",
       "      <td>{'note_id': '302-02', 'patient': '302', 'recor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\n\\nRecord date: 2094-05-01\\n\\nCC: Annual ex...</td>\n",
       "      <td>[{'id': 'P0', 'start': '16', 'end': '26', 'tex...</td>\n",
       "      <td>{'note_id': '327-04', 'patient': '327', 'recor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n\\nRecord date: 2094-06-12\\n\\nJune 12, 2094...</td>\n",
       "      <td>[{'id': 'P0', 'start': '16', 'end': '26', 'tex...</td>\n",
       "      <td>{'note_id': '297-04', 'patient': '297', 'recor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>\\n\\n\\nRecord date: 2135-09-08\\n\\n\\n\\tCARDIOLOG...</td>\n",
       "      <td>[{'id': 'P0', 'start': '16', 'end': '26', 'tex...</td>\n",
       "      <td>{'note_id': '108-04', 'patient': '108', 'recor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>\\n\\n\\nRecord date: 2067-11-24\\n\\n             ...</td>\n",
       "      <td>[{'id': 'P0', 'start': '16', 'end': '26', 'tex...</td>\n",
       "      <td>{'note_id': '103-01', 'patient': '103', 'recor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>\\n\\n\\nRecord date: 2068-01-25\\n\\n             ...</td>\n",
       "      <td>[{'id': 'P0', 'start': '16', 'end': '26', 'tex...</td>\n",
       "      <td>{'note_id': '103-02', 'patient': '103', 'recor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>\\n\\n\\nRecord date: 2068-09-21\\n\\nEDVISIT^84091...</td>\n",
       "      <td>[{'id': 'P0', 'start': '16', 'end': '26', 'tex...</td>\n",
       "      <td>{'note_id': '103-03', 'patient': '103', 'recor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>\\n\\n\\nRecord date: 2072-01-01\\n\\nEDVISIT^84091...</td>\n",
       "      <td>[{'id': 'P0', 'start': '16', 'end': '26', 'tex...</td>\n",
       "      <td>{'note_id': '103-04', 'patient': '103', 'recor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>790 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    \\n\\n\\nRecord date: 2080-11-30\\n\\n\\n\\nReason fo...   \n",
       "1    \\n\\n\\nRecord date: 2061-01-15\\n\\n             ...   \n",
       "2    \\n\\n\\nRecord date: 2083-04-03\\n\\n\\nBeaumont Ho...   \n",
       "3    \\n\\n\\nRecord date: 2094-05-01\\n\\nCC: Annual ex...   \n",
       "4    \\n\\n\\nRecord date: 2094-06-12\\n\\nJune 12, 2094...   \n",
       "..                                                 ...   \n",
       "785  \\n\\n\\nRecord date: 2135-09-08\\n\\n\\n\\tCARDIOLOG...   \n",
       "786  \\n\\n\\nRecord date: 2067-11-24\\n\\n             ...   \n",
       "787  \\n\\n\\nRecord date: 2068-01-25\\n\\n             ...   \n",
       "788  \\n\\n\\nRecord date: 2068-09-21\\n\\nEDVISIT^84091...   \n",
       "789  \\n\\n\\nRecord date: 2072-01-01\\n\\nEDVISIT^84091...   \n",
       "\n",
       "                                                 spans  \\\n",
       "0    [{'id': 'P0', 'start': '16', 'end': '26', 'tex...   \n",
       "1    [{'id': 'P0', 'start': '16', 'end': '26', 'tex...   \n",
       "2    [{'id': 'P0', 'start': '16', 'end': '26', 'tex...   \n",
       "3    [{'id': 'P0', 'start': '16', 'end': '26', 'tex...   \n",
       "4    [{'id': 'P0', 'start': '16', 'end': '26', 'tex...   \n",
       "..                                                 ...   \n",
       "785  [{'id': 'P0', 'start': '16', 'end': '26', 'tex...   \n",
       "786  [{'id': 'P0', 'start': '16', 'end': '26', 'tex...   \n",
       "787  [{'id': 'P0', 'start': '16', 'end': '26', 'tex...   \n",
       "788  [{'id': 'P0', 'start': '16', 'end': '26', 'tex...   \n",
       "789  [{'id': 'P0', 'start': '16', 'end': '26', 'tex...   \n",
       "\n",
       "                                                  meta  \n",
       "0    {'note_id': '369-03', 'patient': '369', 'recor...  \n",
       "1    {'note_id': '299-01', 'patient': '299', 'recor...  \n",
       "2    {'note_id': '302-02', 'patient': '302', 'recor...  \n",
       "3    {'note_id': '327-04', 'patient': '327', 'recor...  \n",
       "4    {'note_id': '297-04', 'patient': '297', 'recor...  \n",
       "..                                                 ...  \n",
       "785  {'note_id': '108-04', 'patient': '108', 'recor...  \n",
       "786  {'note_id': '103-01', 'patient': '103', 'recor...  \n",
       "787  {'note_id': '103-02', 'patient': '103', 'recor...  \n",
       "788  {'note_id': '103-03', 'patient': '103', 'recor...  \n",
       "789  {'note_id': '103-04', 'patient': '103', 'recor...  \n",
       "\n",
       "[790 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3fef6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nRecord date: 2080-11-30\\n\\n\\n\\nReason for Visit\\n\\nOwen is a 63 y/o male here for evaluation of treatment. Doin relatively well. \\n\\n\\n\\nProblems\\n\\n\\n\\n      OA\\n\\n\\n\\n      LLE-PARTIALLY SEVERED-MULT. SURGERIES\\n\\n\\n\\n      IRRIDECTOMY\\n\\n\\n\\n      SKIN ULCER-DR Esposito\\n\\n\\n\\n      PAST SMOKER \\n\\n\\n\\n      HTN\\n\\n\\n\\n\\n\\nMedications\\n\\n\\n\\n      ASA       PO \\n\\n\\n\\n      Vitamin E        PO QD : 400 IU \\n\\n\\n\\n      ATENOLOL   25 MG PO QD\\n\\n\\n\\n      Lipitor (ATORVASTATIN)    10MG,  1 Tablet(s)  PO QD\\n\\n\\n\\n\\n\\nAllergies\\n\\n\\n\\n      NKDA    - NONE\\n\\n\\n\\n\\n\\nNarrative History\\n\\nTakes meds. No SEs. Denies vision change, headache, chest pain, SOB, light head, palptations. Denies loss of balance, strength or sensation. \\n\\n Pulm- no cough.  Occ wheeze. No SOB.\\n\\nGI- no nausea, vomitting, dyspepsia, reflux, abdo pain, diarrhea, constipation, melena, BRBPR.\\n\\nGU- asymp\\n\\nLocomotor- pain left knee/leg. \\n\\nSees Dr Esposito for chr ulcer. Has surgical boot on now. \\n\\nExercise- no\\n\\nDiet-no\\n\\nCigs-no\\n\\nETOH-no\\n\\n\\n\\nExam\\n\\nBP=132/76 , P= 68, Wt= 261 ; NAD,WD, WN\\n\\nHead- no tenderness\\n\\nM&T- moist; no erythema; no exudate; no lesions\\n\\nNeck- supple with no JVD, bruit, LAN or thyromegaly.\\n\\nChest- clear A&P                                                                   Cor- reg rhythm,S1S2 normal with no murmer, gallop or rub\\n\\nAbdo- obese; normal BS;soft with no HSM,mass or tenderness.  DRE- normal sphincter. Prostate small with no nodules. Brown stool.\\n\\nExt- Surg boot LLE. RLE-no edema. \\n\\n\\n\\nAssessment\\n\\nNormotensive. Increased weight. Did have elevated glucose last visit. \\n\\n\\n\\nDisposition and Plans\\n\\nCBC,glu,PSA. Decrease weight- increase exercise and eat less. Cont meds. RTC 6 mon or PRN.\\n\\n______________________________\\n\\n William Seth Potter, M.D.\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = data['text'].tolist() # type: ignore\n",
    "test_texts = test_data['text'].tolist() # type: ignore\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d14d702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'P0',\n",
       "  'start': '16',\n",
       "  'end': '26',\n",
       "  'text': '2080-11-30',\n",
       "  'TYPE': 'DATE',\n",
       "  'comment': '',\n",
       "  'label': 'DATE'},\n",
       " {'id': 'P1',\n",
       "  'start': '48',\n",
       "  'end': '52',\n",
       "  'text': 'Owen',\n",
       "  'TYPE': 'PATIENT',\n",
       "  'comment': '',\n",
       "  'label': 'PATIENT'},\n",
       " {'id': 'P2',\n",
       "  'start': '58',\n",
       "  'end': '60',\n",
       "  'text': '63',\n",
       "  'TYPE': 'AGE',\n",
       "  'comment': '',\n",
       "  'label': 'AGE'},\n",
       " {'id': 'P3',\n",
       "  'start': '242',\n",
       "  'end': '250',\n",
       "  'text': 'Esposito',\n",
       "  'TYPE': 'DOCTOR',\n",
       "  'comment': '',\n",
       "  'label': 'STAFF'},\n",
       " {'id': 'P4',\n",
       "  'start': '854',\n",
       "  'end': '862',\n",
       "  'text': 'Esposito',\n",
       "  'TYPE': 'DOCTOR',\n",
       "  'comment': '',\n",
       "  'label': 'STAFF'},\n",
       " {'id': 'P5',\n",
       "  'start': '1664',\n",
       "  'end': '1683',\n",
       "  'text': 'William Seth Potter',\n",
       "  'TYPE': 'DOCTOR',\n",
       "  'comment': '',\n",
       "  'label': 'STAFF'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spans = data['spans'].tolist() # type: ignore\n",
    "test_spans = test_data['spans'].tolist() # type: ignore\n",
    "spans[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70cbf1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nm = 'microsoft/deberta-v3-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a87e6e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manibatra/mambaforge/envs/phi/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/Users/manibatra/mambaforge/envs/phi/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, convert_slow_tokenizer\n",
    "tokz = AutoTokenizer.from_pretrained(model_nm, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40d73d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_function(x): return tokz(x['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8d594c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "ds = Dataset.from_pandas(data)\n",
    "test_ds = Dataset.from_pandas(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdf4b285",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'spans', 'meta', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 790\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_ds = ds.map(tok_function, batched=True)\n",
    "test_tok_ds = test_ds.map(tok_function, batched=True)\n",
    "tok_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "913a464b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 9737,\n",
       " 1043,\n",
       " 294,\n",
       " 89553,\n",
       " 271,\n",
       " 1554,\n",
       " 271,\n",
       " 1076,\n",
       " 18037,\n",
       " 270,\n",
       " 3847,\n",
       " 12980,\n",
       " 269,\n",
       " 266,\n",
       " 7592,\n",
       " 2982,\n",
       " 320,\n",
       " 795,\n",
       " 2844,\n",
       " 422,\n",
       " 270,\n",
       " 4291,\n",
       " 265,\n",
       " 1035,\n",
       " 260,\n",
       " 771,\n",
       " 547,\n",
       " 2936,\n",
       " 371,\n",
       " 260,\n",
       " 16067,\n",
       " 27122,\n",
       " 1093,\n",
       " 14360,\n",
       " 271,\n",
       " 71889,\n",
       " 476,\n",
       " 43993,\n",
       " 662,\n",
       " 45227,\n",
       " 4272,\n",
       " 271,\n",
       " 1135,\n",
       " 55111,\n",
       " 260,\n",
       " 662,\n",
       " 96700,\n",
       " 41800,\n",
       " 273,\n",
       " 81084,\n",
       " 50938,\n",
       " 67607,\n",
       " 3021,\n",
       " 67958,\n",
       " 23042,\n",
       " 68148,\n",
       " 271,\n",
       " 15746,\n",
       " 74543,\n",
       " 68452,\n",
       " 109013,\n",
       " 1349,\n",
       " 1233,\n",
       " 22438,\n",
       " 63834,\n",
       " 30561,\n",
       " 15511,\n",
       " 10397,\n",
       " 829,\n",
       " 15511,\n",
       " 78381,\n",
       " 877,\n",
       " 3755,\n",
       " 30840,\n",
       " 336,\n",
       " 34823,\n",
       " 57903,\n",
       " 1502,\n",
       " 977,\n",
       " 15502,\n",
       " 15511,\n",
       " 78381,\n",
       " 115162,\n",
       " 287,\n",
       " 40883,\n",
       " 14634,\n",
       " 55275,\n",
       " 10383,\n",
       " 285,\n",
       " 466,\n",
       " 24393,\n",
       " 261,\n",
       " 376,\n",
       " 20587,\n",
       " 555,\n",
       " 268,\n",
       " 285,\n",
       " 15511,\n",
       " 78381,\n",
       " 79673,\n",
       " 27507,\n",
       " 13975,\n",
       " 341,\n",
       " 69897,\n",
       " 31775,\n",
       " 3401,\n",
       " 33315,\n",
       " 20542,\n",
       " 260,\n",
       " 585,\n",
       " 8628,\n",
       " 268,\n",
       " 260,\n",
       " 11289,\n",
       " 3933,\n",
       " 2345,\n",
       " 575,\n",
       " 261,\n",
       " 11141,\n",
       " 261,\n",
       " 4458,\n",
       " 1427,\n",
       " 261,\n",
       " 97273,\n",
       " 261,\n",
       " 731,\n",
       " 761,\n",
       " 261,\n",
       " 77214,\n",
       " 27592,\n",
       " 268,\n",
       " 260,\n",
       " 11289,\n",
       " 3933,\n",
       " 1265,\n",
       " 265,\n",
       " 1990,\n",
       " 261,\n",
       " 2067,\n",
       " 289,\n",
       " 10261,\n",
       " 260,\n",
       " 32742,\n",
       " 358,\n",
       " 271,\n",
       " 363,\n",
       " 15108,\n",
       " 260,\n",
       " 67283,\n",
       " 100083,\n",
       " 260,\n",
       " 585,\n",
       " 97273,\n",
       " 260,\n",
       " 16034,\n",
       " 271,\n",
       " 363,\n",
       " 15501,\n",
       " 261,\n",
       " 31566,\n",
       " 5654,\n",
       " 261,\n",
       " 117130,\n",
       " 261,\n",
       " 27827,\n",
       " 261,\n",
       " 25191,\n",
       " 2965,\n",
       " 1427,\n",
       " 261,\n",
       " 18321,\n",
       " 261,\n",
       " 26000,\n",
       " 261,\n",
       " 351,\n",
       " 37888,\n",
       " 261,\n",
       " 16256,\n",
       " 983,\n",
       " 15391,\n",
       " 260,\n",
       " 27636,\n",
       " 271,\n",
       " 283,\n",
       " 608,\n",
       " 11879,\n",
       " 48546,\n",
       " 27043,\n",
       " 271,\n",
       " 1427,\n",
       " 595,\n",
       " 5188,\n",
       " 320,\n",
       " 17703,\n",
       " 260,\n",
       " 1734,\n",
       " 268,\n",
       " 1011,\n",
       " 74543,\n",
       " 270,\n",
       " 90696,\n",
       " 39618,\n",
       " 260,\n",
       " 4638,\n",
       " 7425,\n",
       " 5897,\n",
       " 277,\n",
       " 394,\n",
       " 260,\n",
       " 14508,\n",
       " 271,\n",
       " 363,\n",
       " 11398,\n",
       " 271,\n",
       " 1967,\n",
       " 716,\n",
       " 8843,\n",
       " 268,\n",
       " 271,\n",
       " 1967,\n",
       " 9000,\n",
       " 17467,\n",
       " 271,\n",
       " 1967,\n",
       " 9265,\n",
       " 13060,\n",
       " 1510,\n",
       " 25828,\n",
       " 320,\n",
       " 9212,\n",
       " 366,\n",
       " 916,\n",
       " 1510,\n",
       " 7614,\n",
       " 261,\n",
       " 1462,\n",
       " 297,\n",
       " 1510,\n",
       " 43273,\n",
       " 2600,\n",
       " 52609,\n",
       " 261,\n",
       " 16953,\n",
       " 261,\n",
       " 50988,\n",
       " 3796,\n",
       " 271,\n",
       " 363,\n",
       " 30400,\n",
       " 749,\n",
       " 974,\n",
       " 1193,\n",
       " 271,\n",
       " 13745,\n",
       " 346,\n",
       " 363,\n",
       " 88304,\n",
       " 346,\n",
       " 363,\n",
       " 113296,\n",
       " 346,\n",
       " 363,\n",
       " 17507,\n",
       " 19436,\n",
       " 271,\n",
       " 31135,\n",
       " 275,\n",
       " 363,\n",
       " 29490,\n",
       " 691,\n",
       " 261,\n",
       " 39416,\n",
       " 1632,\n",
       " 261,\n",
       " 17957,\n",
       " 289,\n",
       " 10721,\n",
       " 834,\n",
       " 85758,\n",
       " 701,\n",
       " 260,\n",
       " 24382,\n",
       " 271,\n",
       " 913,\n",
       " 336,\n",
       " 974,\n",
       " 1230,\n",
       " 11484,\n",
       " 271,\n",
       " 22365,\n",
       " 9093,\n",
       " 261,\n",
       " 430,\n",
       " 435,\n",
       " 430,\n",
       " 445,\n",
       " 1697,\n",
       " 275,\n",
       " 363,\n",
       " 42543,\n",
       " 7403,\n",
       " 261,\n",
       " 61484,\n",
       " 289,\n",
       " 11543,\n",
       " 76069,\n",
       " 271,\n",
       " 18578,\n",
       " 346,\n",
       " 1697,\n",
       " 12401,\n",
       " 346,\n",
       " 12068,\n",
       " 275,\n",
       " 363,\n",
       " 83144,\n",
       " 261,\n",
       " 27536,\n",
       " 289,\n",
       " 30400,\n",
       " 260,\n",
       " 85386,\n",
       " 271,\n",
       " 1697,\n",
       " 81129,\n",
       " 260,\n",
       " 46425,\n",
       " 536,\n",
       " 275,\n",
       " 363,\n",
       " 56726,\n",
       " 260,\n",
       " 2743,\n",
       " 13725,\n",
       " 260,\n",
       " 37959,\n",
       " 271,\n",
       " 32748,\n",
       " 5897,\n",
       " 1093,\n",
       " 14360,\n",
       " 260,\n",
       " 909,\n",
       " 14360,\n",
       " 271,\n",
       " 1967,\n",
       " 39259,\n",
       " 260,\n",
       " 10154,\n",
       " 31301,\n",
       " 79664,\n",
       " 35667,\n",
       " 260,\n",
       " 21772,\n",
       " 1272,\n",
       " 260,\n",
       " 2709,\n",
       " 286,\n",
       " 8560,\n",
       " 10231,\n",
       " 437,\n",
       " 836,\n",
       " 260,\n",
       " 92690,\n",
       " 263,\n",
       " 8376,\n",
       " 19548,\n",
       " 261,\n",
       " 77140,\n",
       " 261,\n",
       " 56098,\n",
       " 260,\n",
       " 67586,\n",
       " 1272,\n",
       " 271,\n",
       " 993,\n",
       " 2111,\n",
       " 263,\n",
       " 1672,\n",
       " 625,\n",
       " 260,\n",
       " 68083,\n",
       " 20542,\n",
       " 260,\n",
       " 63349,\n",
       " 525,\n",
       " 18528,\n",
       " 289,\n",
       " 6181,\n",
       " 1609,\n",
       " 260,\n",
       " 5179,\n",
       " 616,\n",
       " 616,\n",
       " 616,\n",
       " 616,\n",
       " 616,\n",
       " 616,\n",
       " 616,\n",
       " 616,\n",
       " 616,\n",
       " 616,\n",
       " 616,\n",
       " 616,\n",
       " 616,\n",
       " 616,\n",
       " 616,\n",
       " 616,\n",
       " 616,\n",
       " 616,\n",
       " 616,\n",
       " 616,\n",
       " 616,\n",
       " 616,\n",
       " 616,\n",
       " 616,\n",
       " 616,\n",
       " 616,\n",
       " 616,\n",
       " 616,\n",
       " 616,\n",
       " 2833,\n",
       " 13987,\n",
       " 9918,\n",
       " 261,\n",
       " 749,\n",
       " 260,\n",
       " 691,\n",
       " 260,\n",
       " 2]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_ds['text'][0]\n",
    "tok_ds['input_ids'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "656e01fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [{'id': 'P0', 'start': '16', 'end': '26', 'tex...\n",
       "1      [{'id': 'P0', 'start': '16', 'end': '26', 'tex...\n",
       "2      [{'id': 'P0', 'start': '16', 'end': '26', 'tex...\n",
       "3      [{'id': 'P0', 'start': '16', 'end': '26', 'tex...\n",
       "4      [{'id': 'P0', 'start': '16', 'end': '26', 'tex...\n",
       "                             ...                        \n",
       "785    [{'id': 'P0', 'start': '16', 'end': '26', 'tex...\n",
       "786    [{'id': 'P0', 'start': '16', 'end': '26', 'tex...\n",
       "787    [{'id': 'P0', 'start': '16', 'end': '26', 'tex...\n",
       "788    [{'id': 'P0', 'start': '16', 'end': '26', 'tex...\n",
       "789    [{'id': 'P0', 'start': '16', 'end': '26', 'tex...\n",
       "Name: spans, Length: 790, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['spans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "278a0980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]: 1: (0, 0)\n",
      "▁Record: 9737: (3, 9)\n",
      "▁date: 1043: (9, 14)\n",
      ":: 294: (14, 15)\n",
      "▁2080: 89553: (15, 20)\n",
      "-: 271: (20, 21)\n",
      "11: 1554: (21, 23)\n",
      "-: 271: (23, 24)\n",
      "30: 1076: (24, 26)\n",
      "▁Reason: 18037: (29, 36)\n",
      "▁for: 270: (36, 40)\n",
      "▁Visit: 3847: (40, 46)\n",
      "▁Owen: 12980: (47, 52)\n",
      "▁is: 269: (52, 55)\n",
      "▁a: 266: (55, 57)\n",
      "▁63: 7592: (57, 60)\n",
      "▁y: 2982: (60, 62)\n",
      "/: 320: (62, 63)\n",
      "o: 795: (63, 64)\n",
      "▁male: 2844: (64, 69)\n",
      "▁here: 422: (69, 74)\n",
      "▁for: 270: (74, 78)\n",
      "▁evaluation: 4291: (78, 89)\n",
      "▁of: 265: (89, 92)\n",
      "▁treatment: 1035: (92, 102)\n",
      ".: 260: (102, 103)\n",
      "▁Do: 771: (103, 106)\n",
      "in: 547: (106, 108)\n",
      "▁relatively: 2936: (108, 119)\n",
      "▁well: 371: (119, 124)\n",
      ".: 260: (124, 125)\n",
      "▁Problems: 16067: (129, 138)\n",
      "▁OA: 27122: (147, 150)\n",
      "▁L: 1093: (159, 161)\n",
      "LE: 14360: (161, 163)\n",
      "-: 271: (163, 164)\n",
      "PART: 71889: (164, 168)\n",
      "I: 476: (168, 169)\n",
      "ALLY: 43993: (169, 173)\n",
      "▁S: 662: (173, 175)\n",
      "EVER: 45227: (175, 179)\n",
      "ED: 4272: (179, 181)\n",
      "-: 271: (181, 182)\n",
      "M: 1135: (182, 183)\n",
      "ULT: 55111: (183, 186)\n",
      ".: 260: (186, 187)\n",
      "▁S: 662: (187, 189)\n",
      "URGE: 96700: (189, 193)\n",
      "RIES: 41800: (193, 197)\n",
      "▁I: 273: (206, 208)\n",
      "RRI: 81084: (208, 211)\n",
      "DEC: 50938: (211, 214)\n",
      "TOM: 67607: (214, 217)\n",
      "Y: 3021: (217, 218)\n",
      "▁SKIN: 67958: (227, 232)\n",
      "▁UL: 23042: (232, 235)\n",
      "CER: 68148: (235, 238)\n",
      "-: 271: (238, 239)\n",
      "DR: 15746: (239, 241)\n",
      "▁Esposito: 74543: (241, 250)\n",
      "▁PAST: 68452: (259, 264)\n",
      "▁SMOKE: 109013: (264, 270)\n",
      "R: 1349: (270, 271)\n",
      "▁H: 1233: (281, 283)\n",
      "TN: 22438: (283, 285)\n",
      "▁Medications: 63834: (290, 302)\n",
      "▁ASA: 30561: (311, 315)\n",
      "▁PO: 15511: (321, 324)\n",
      "▁Vitamin: 10397: (334, 342)\n",
      "▁E: 829: (342, 344)\n",
      "▁PO: 15511: (351, 354)\n",
      "▁QD: 78381: (354, 357)\n",
      "▁:: 877: (357, 359)\n",
      "▁400: 3755: (359, 363)\n",
      "▁IU: 30840: (363, 366)\n",
      "▁A: 336: (376, 378)\n",
      "TEN: 34823: (378, 381)\n",
      "OLO: 57903: (381, 384)\n",
      "L: 1502: (384, 385)\n",
      "▁25: 977: (387, 390)\n",
      "▁MG: 15502: (390, 393)\n",
      "▁PO: 15511: (393, 396)\n",
      "▁QD: 78381: (396, 399)\n",
      "▁Lipitor: 115162: (408, 416)\n",
      "▁(: 287: (416, 418)\n",
      "ATOR: 40883: (418, 422)\n",
      "VA: 14634: (422, 424)\n",
      "STAT: 55275: (424, 428)\n",
      "IN: 10383: (428, 430)\n",
      "): 285: (430, 431)\n",
      "▁10: 466: (434, 437)\n",
      "MG: 24393: (437, 439)\n",
      ",: 261: (439, 440)\n",
      "▁1: 376: (441, 443)\n",
      "▁Tablet: 20587: (443, 450)\n",
      "(: 555: (450, 451)\n",
      "s: 268: (451, 452)\n",
      "): 285: (452, 453)\n",
      "▁PO: 15511: (454, 457)\n",
      "▁QD: 78381: (457, 460)\n",
      "▁Allergies: 79673: (465, 475)\n",
      "▁NK: 27507: (484, 487)\n",
      "DA: 13975: (487, 489)\n",
      "▁-: 341: (492, 494)\n",
      "▁NONE: 69897: (494, 499)\n",
      "▁Narrative: 31775: (504, 514)\n",
      "▁History: 3401: (514, 522)\n",
      "▁Takes: 33315: (523, 529)\n",
      "▁meds: 20542: (529, 534)\n",
      ".: 260: (534, 535)\n",
      "▁No: 585: (535, 538)\n",
      "▁SE: 8628: (538, 541)\n",
      "s: 268: (541, 542)\n",
      ".: 260: (542, 543)\n",
      "▁Den: 11289: (543, 547)\n",
      "ies: 3933: (547, 550)\n",
      "▁vision: 2345: (550, 557)\n",
      "▁change: 575: (557, 564)\n",
      ",: 261: (564, 565)\n",
      "▁headache: 11141: (565, 574)\n",
      ",: 261: (574, 575)\n",
      "▁chest: 4458: (575, 581)\n",
      "▁pain: 1427: (581, 586)\n",
      ",: 261: (586, 587)\n",
      "▁SOB: 97273: (587, 591)\n",
      ",: 261: (591, 592)\n",
      "▁light: 731: (592, 598)\n",
      "▁head: 761: (598, 603)\n",
      ",: 261: (603, 604)\n",
      "▁palp: 77214: (604, 609)\n",
      "tation: 27592: (609, 615)\n",
      "s: 268: (615, 616)\n",
      ".: 260: (616, 617)\n",
      "▁Den: 11289: (617, 621)\n",
      "ies: 3933: (621, 624)\n",
      "▁loss: 1265: (624, 629)\n",
      "▁of: 265: (629, 632)\n",
      "▁balance: 1990: (632, 640)\n",
      ",: 261: (640, 641)\n",
      "▁strength: 2067: (641, 650)\n",
      "▁or: 289: (650, 653)\n",
      "▁sensation: 10261: (653, 663)\n",
      ".: 260: (663, 664)\n",
      "▁Pul: 32742: (667, 671)\n",
      "m: 358: (671, 672)\n",
      "-: 271: (672, 673)\n",
      "▁no: 363: (673, 676)\n",
      "▁cough: 15108: (676, 682)\n",
      ".: 260: (682, 683)\n",
      "▁Occ: 67283: (684, 688)\n",
      "▁wheeze: 100083: (688, 695)\n",
      ".: 260: (695, 696)\n",
      "▁No: 585: (696, 699)\n",
      "▁SOB: 97273: (699, 703)\n",
      ".: 260: (703, 704)\n",
      "▁GI: 16034: (705, 708)\n",
      "-: 271: (708, 709)\n",
      "▁no: 363: (709, 712)\n",
      "▁nausea: 15501: (712, 719)\n",
      ",: 261: (719, 720)\n",
      "▁vomit: 31566: (720, 726)\n",
      "ting: 5654: (726, 730)\n",
      ",: 261: (730, 731)\n",
      "▁dyspepsia: 117130: (731, 741)\n",
      ",: 261: (741, 742)\n",
      "▁reflux: 27827: (742, 749)\n",
      ",: 261: (749, 750)\n",
      "▁ab: 25191: (750, 753)\n",
      "do: 2965: (753, 755)\n",
      "▁pain: 1427: (755, 760)\n",
      ",: 261: (760, 761)\n",
      "▁diarrhea: 18321: (761, 770)\n",
      ",: 261: (770, 771)\n",
      "▁constipation: 26000: (771, 784)\n",
      ",: 261: (784, 785)\n",
      "▁me: 351: (785, 788)\n",
      "lena: 37888: (788, 792)\n",
      ",: 261: (792, 793)\n",
      "▁BR: 16256: (793, 796)\n",
      "B: 983: (796, 797)\n",
      "PR: 15391: (797, 799)\n",
      ".: 260: (799, 800)\n",
      "▁GU: 27636: (801, 804)\n",
      "-: 271: (804, 805)\n",
      "▁as: 283: (805, 808)\n",
      "y: 608: (808, 809)\n",
      "mp: 11879: (809, 811)\n",
      "▁Loco: 48546: (812, 817)\n",
      "motor: 27043: (817, 822)\n",
      "-: 271: (822, 823)\n",
      "▁pain: 1427: (823, 828)\n",
      "▁left: 595: (828, 833)\n",
      "▁knee: 5188: (833, 838)\n",
      "/: 320: (838, 839)\n",
      "leg: 17703: (839, 842)\n",
      ".: 260: (842, 843)\n",
      "▁See: 1734: (845, 849)\n",
      "s: 268: (849, 850)\n",
      "▁Dr: 1011: (850, 853)\n",
      "▁Esposito: 74543: (853, 862)\n",
      "▁for: 270: (862, 866)\n",
      "▁chr: 90696: (866, 870)\n",
      "▁ulcer: 39618: (870, 876)\n",
      ".: 260: (876, 877)\n",
      "▁Has: 4638: (877, 881)\n",
      "▁surgical: 7425: (881, 890)\n",
      "▁boot: 5897: (890, 895)\n",
      "▁on: 277: (895, 898)\n",
      "▁now: 394: (898, 902)\n",
      ".: 260: (902, 903)\n",
      "▁Exercise: 14508: (905, 914)\n",
      "-: 271: (914, 915)\n",
      "▁no: 363: (915, 918)\n",
      "▁Diet: 11398: (919, 924)\n",
      "-: 271: (924, 925)\n",
      "no: 1967: (925, 927)\n",
      "▁C: 716: (928, 930)\n",
      "ig: 8843: (930, 932)\n",
      "s: 268: (932, 933)\n",
      "-: 271: (933, 934)\n",
      "no: 1967: (934, 936)\n",
      "▁ET: 9000: (937, 940)\n",
      "OH: 17467: (940, 942)\n",
      "-: 271: (942, 943)\n",
      "no: 1967: (943, 945)\n",
      "▁Exam: 9265: (948, 953)\n",
      "▁BP: 13060: (954, 957)\n",
      "=: 1510: (957, 958)\n",
      "132: 25828: (958, 961)\n",
      "/: 320: (961, 962)\n",
      "76: 9212: (962, 964)\n",
      "▁,: 366: (964, 966)\n",
      "▁P: 916: (966, 968)\n",
      "=: 1510: (968, 969)\n",
      "▁68: 7614: (969, 972)\n",
      ",: 261: (972, 973)\n",
      "▁W: 1462: (973, 975)\n",
      "t: 297: (975, 976)\n",
      "=: 1510: (976, 977)\n",
      "▁261: 43273: (977, 981)\n",
      "▁;: 2600: (981, 983)\n",
      "▁NAD: 52609: (983, 987)\n",
      ",: 261: (987, 988)\n",
      "WD: 16953: (988, 990)\n",
      ",: 261: (990, 991)\n",
      "▁WN: 50988: (991, 994)\n",
      "▁Head: 3796: (995, 1000)\n",
      "-: 271: (1000, 1001)\n",
      "▁no: 363: (1001, 1004)\n",
      "▁tenderness: 30400: (1004, 1015)\n",
      "▁M: 749: (1016, 1018)\n",
      "&: 974: (1018, 1019)\n",
      "T: 1193: (1019, 1020)\n",
      "-: 271: (1020, 1021)\n",
      "▁moist: 13745: (1021, 1027)\n",
      ";: 346: (1027, 1028)\n",
      "▁no: 363: (1028, 1031)\n",
      "▁erythema: 88304: (1031, 1040)\n",
      ";: 346: (1040, 1041)\n",
      "▁no: 363: (1041, 1044)\n",
      "▁exudate: 113296: (1044, 1052)\n",
      ";: 346: (1052, 1053)\n",
      "▁no: 363: (1053, 1056)\n",
      "▁lesions: 17507: (1056, 1064)\n",
      "▁Neck: 19436: (1065, 1070)\n",
      "-: 271: (1070, 1071)\n",
      "▁supple: 31135: (1071, 1078)\n",
      "▁with: 275: (1078, 1083)\n",
      "▁no: 363: (1083, 1086)\n",
      "▁JV: 29490: (1086, 1089)\n",
      "D: 691: (1089, 1090)\n",
      ",: 261: (1090, 1091)\n",
      "▁bru: 39416: (1091, 1095)\n",
      "it: 1632: (1095, 1097)\n",
      ",: 261: (1097, 1098)\n",
      "▁LAN: 17957: (1098, 1102)\n",
      "▁or: 289: (1102, 1105)\n",
      "▁thy: 10721: (1105, 1109)\n",
      "r: 834: (1109, 1110)\n",
      "omega: 85758: (1110, 1115)\n",
      "ly: 701: (1115, 1117)\n",
      ".: 260: (1117, 1118)\n",
      "▁Chest: 24382: (1119, 1125)\n",
      "-: 271: (1125, 1126)\n",
      "▁clear: 913: (1126, 1132)\n",
      "▁A: 336: (1132, 1134)\n",
      "&: 974: (1134, 1135)\n",
      "P: 1230: (1135, 1136)\n",
      "▁Cor: 11484: (1202, 1206)\n",
      "-: 271: (1206, 1207)\n",
      "▁reg: 22365: (1207, 1211)\n",
      "▁rhythm: 9093: (1211, 1218)\n",
      ",: 261: (1218, 1219)\n",
      "S: 430: (1219, 1220)\n",
      "1: 435: (1220, 1221)\n",
      "S: 430: (1221, 1222)\n",
      "2: 445: (1222, 1223)\n",
      "▁normal: 1697: (1223, 1230)\n",
      "▁with: 275: (1230, 1235)\n",
      "▁no: 363: (1235, 1238)\n",
      "▁mur: 42543: (1238, 1242)\n",
      "mer: 7403: (1242, 1245)\n",
      ",: 261: (1245, 1246)\n",
      "▁gallop: 61484: (1246, 1253)\n",
      "▁or: 289: (1253, 1256)\n",
      "▁rub: 11543: (1256, 1260)\n",
      "▁Abdo: 76069: (1261, 1266)\n",
      "-: 271: (1266, 1267)\n",
      "▁obese: 18578: (1267, 1273)\n",
      ";: 346: (1273, 1274)\n",
      "▁normal: 1697: (1274, 1281)\n",
      "▁BS: 12401: (1281, 1284)\n",
      ";: 346: (1284, 1285)\n",
      "soft: 12068: (1285, 1289)\n",
      "▁with: 275: (1289, 1294)\n",
      "▁no: 363: (1294, 1297)\n",
      "▁HSM: 83144: (1297, 1301)\n",
      ",: 261: (1301, 1302)\n",
      "mass: 27536: (1302, 1306)\n",
      "▁or: 289: (1306, 1309)\n",
      "▁tenderness: 30400: (1309, 1320)\n",
      ".: 260: (1320, 1321)\n",
      "▁DRE: 85386: (1322, 1326)\n",
      "-: 271: (1326, 1327)\n",
      "▁normal: 1697: (1327, 1334)\n",
      "▁sphincter: 81129: (1334, 1344)\n",
      ".: 260: (1344, 1345)\n",
      "▁Prostate: 46425: (1345, 1354)\n",
      "▁small: 536: (1354, 1360)\n",
      "▁with: 275: (1360, 1365)\n",
      "▁no: 363: (1365, 1368)\n",
      "▁nodules: 56726: (1368, 1376)\n",
      ".: 260: (1376, 1377)\n",
      "▁Brown: 2743: (1377, 1383)\n",
      "▁stool: 13725: (1383, 1389)\n",
      ".: 260: (1389, 1390)\n",
      "▁Ext: 37959: (1391, 1395)\n",
      "-: 271: (1395, 1396)\n",
      "▁Surg: 32748: (1396, 1401)\n",
      "▁boot: 5897: (1401, 1406)\n",
      "▁L: 1093: (1406, 1408)\n",
      "LE: 14360: (1408, 1410)\n",
      ".: 260: (1410, 1411)\n",
      "▁R: 909: (1411, 1413)\n",
      "LE: 14360: (1413, 1415)\n",
      "-: 271: (1415, 1416)\n",
      "no: 1967: (1416, 1418)\n",
      "▁edema: 39259: (1418, 1424)\n",
      ".: 260: (1424, 1425)\n",
      "▁Assessment: 10154: (1429, 1440)\n",
      "▁Norm: 31301: (1441, 1446)\n",
      "oten: 79664: (1446, 1450)\n",
      "sive: 35667: (1450, 1454)\n",
      ".: 260: (1454, 1455)\n",
      "▁Increased: 21772: (1455, 1465)\n",
      "▁weight: 1272: (1465, 1472)\n",
      ".: 260: (1472, 1473)\n",
      "▁Did: 2709: (1473, 1477)\n",
      "▁have: 286: (1477, 1482)\n",
      "▁elevated: 8560: (1482, 1491)\n",
      "▁glucose: 10231: (1491, 1499)\n",
      "▁last: 437: (1499, 1504)\n",
      "▁visit: 836: (1504, 1510)\n",
      ".: 260: (1510, 1511)\n",
      "▁Disposition: 92690: (1515, 1527)\n",
      "▁and: 263: (1527, 1531)\n",
      "▁Plans: 8376: (1531, 1537)\n",
      "▁CBC: 19548: (1538, 1542)\n",
      ",: 261: (1542, 1543)\n",
      "glu: 77140: (1543, 1546)\n",
      ",: 261: (1546, 1547)\n",
      "PSA: 56098: (1547, 1550)\n",
      ".: 260: (1550, 1551)\n",
      "▁Decrease: 67586: (1551, 1560)\n",
      "▁weight: 1272: (1560, 1567)\n",
      "-: 271: (1567, 1568)\n",
      "▁increase: 993: (1568, 1577)\n",
      "▁exercise: 2111: (1577, 1586)\n",
      "▁and: 263: (1586, 1590)\n",
      "▁eat: 1672: (1590, 1594)\n",
      "▁less: 625: (1594, 1599)\n",
      ".: 260: (1599, 1600)\n",
      "▁Cont: 68083: (1600, 1605)\n",
      "▁meds: 20542: (1605, 1610)\n",
      ".: 260: (1610, 1611)\n",
      "▁RTC: 63349: (1611, 1615)\n",
      "▁6: 525: (1615, 1617)\n",
      "▁mon: 18528: (1617, 1621)\n",
      "▁or: 289: (1621, 1624)\n",
      "▁PR: 6181: (1624, 1627)\n",
      "N: 1609: (1627, 1628)\n",
      ".: 260: (1628, 1629)\n",
      "▁_: 5179: (1630, 1632)\n",
      "_: 616: (1632, 1633)\n",
      "_: 616: (1633, 1634)\n",
      "_: 616: (1634, 1635)\n",
      "_: 616: (1635, 1636)\n",
      "_: 616: (1636, 1637)\n",
      "_: 616: (1637, 1638)\n",
      "_: 616: (1638, 1639)\n",
      "_: 616: (1639, 1640)\n",
      "_: 616: (1640, 1641)\n",
      "_: 616: (1641, 1642)\n",
      "_: 616: (1642, 1643)\n",
      "_: 616: (1643, 1644)\n",
      "_: 616: (1644, 1645)\n",
      "_: 616: (1645, 1646)\n",
      "_: 616: (1646, 1647)\n",
      "_: 616: (1647, 1648)\n",
      "_: 616: (1648, 1649)\n",
      "_: 616: (1649, 1650)\n",
      "_: 616: (1650, 1651)\n",
      "_: 616: (1651, 1652)\n",
      "_: 616: (1652, 1653)\n",
      "_: 616: (1653, 1654)\n",
      "_: 616: (1654, 1655)\n",
      "_: 616: (1655, 1656)\n",
      "_: 616: (1656, 1657)\n",
      "_: 616: (1657, 1658)\n",
      "_: 616: (1658, 1659)\n",
      "_: 616: (1659, 1660)\n",
      "_: 616: (1660, 1661)\n",
      "▁William: 2833: (1663, 1671)\n",
      "▁Seth: 13987: (1671, 1676)\n",
      "▁Potter: 9918: (1676, 1683)\n",
      ",: 261: (1683, 1684)\n",
      "▁M: 749: (1684, 1686)\n",
      ".: 260: (1686, 1687)\n",
      "D: 691: (1687, 1688)\n",
      ".: 260: (1688, 1689)\n",
      "[SEP]: 2: (0, 0)\n"
     ]
    }
   ],
   "source": [
    "input_ids = tok_ds['input_ids'][0]\n",
    "tokens = tokz.convert_ids_to_tokens(input_ids)\n",
    "token_positions = tokz(tok_ds['text'][0], return_offsets_mapping=True)['offset_mapping']\n",
    "\n",
    "for token, input_id, token_pos in zip(tokens, input_ids, token_positions):\n",
    "    print(f'{token}: {input_id}: {token_pos}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58251d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    \"O\": 0,\n",
    "    \"B-DATE\": 1,\n",
    "    \"I-DATE\": 2,\n",
    "    \"B-PATIENT\": 3,\n",
    "    \"I-PATIENT\": 4,\n",
    "    \"B-AGE\": 5,\n",
    "    \"I-AGE\": 6,\n",
    "    \"B-STAFF\": 7,\n",
    "    \"I-STAFF\": 8,\n",
    "    \"B-PHONE\": 9,\n",
    "    \"I-PHONE\": 10,\n",
    "    \"B-EMAIL\": 11,\n",
    "    \"I-EMAIL\": 12,\n",
    "    \"B-ID\": 13,\n",
    "    \"I-ID\": 14,\n",
    "    \"B-HOSP\": 15,\n",
    "    \"I-HOSP\": 16,\n",
    "    \"B-PATORG\": 17,\n",
    "    \"I-PATORG\": 18,\n",
    "    \"B-LOC\": 19,\n",
    "    \"I-LOC\": 20,\n",
    "    \"B-OTHERPHI\": 21,\n",
    "    \"I-OTHERPHI\": 22,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b4522ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_data(text, spans, input_ids):\n",
    "    tokens = tokz.convert_ids_to_tokens(input_ids)\n",
    "    labels = ['O'] * len(input_ids)\n",
    "    token_positions = tokz(text, return_offsets_mapping=True)['offset_mapping']\n",
    "\n",
    "\n",
    "    for span in spans:\n",
    "        start, end, label = span['start'], span['end'], span['label']\n",
    "\n",
    "        token_start, token_end = None, None\n",
    "\n",
    "        for idx, (char_start, char_end) in enumerate(token_positions):\n",
    "            if tokens[idx].startswith('▁'):\n",
    "                char_start += 1\n",
    "            # print(tokens[idx], char_start, char_end, start, end)\n",
    "            if char_start == int(start):\n",
    "                token_start = idx\n",
    "            if char_end == int(end):\n",
    "                token_end = idx\n",
    "                break\n",
    "        \n",
    "        if token_start is not None and token_end is not None:\n",
    "            # print(token_start, token_end, label)\n",
    "            labels[token_start] = f'B-{label}'\n",
    "            for idx in range(token_start + 1, token_end + 1):\n",
    "                labels[idx] = f'I-{label}'\n",
    "\n",
    "    # input_ids = tokz.convert_tokens_to_ids(tokens)\n",
    "    label_ids = [label_map[label] for label in labels]\n",
    "\n",
    "    return label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "988c19f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_ds_processed = tok_ds.add_column('labels', [pre_process_data(text, spans, input_ids) for text, spans, input_ids in zip(tok_ds['text'], tok_ds['spans'], tok_ds['input_ids'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c364441",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tok_ds_processed = test_tok_ds.add_column('labels', [pre_process_data(text, spans, input_ids) for text, spans, input_ids in zip(test_tok_ds['text'], test_tok_ds['spans'], test_tok_ds['input_ids'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4cf791d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] - O\n",
      "▁Record - O\n",
      "▁date - O\n",
      ": - O\n",
      "▁20 - B-DATE\n",
      "61 - I-DATE\n",
      "- - I-DATE\n",
      "01 - I-DATE\n",
      "- - I-DATE\n",
      "15 - I-DATE\n",
      "▁ST - B-HOSP\n",
      "▁LUCI - I-HOSP\n",
      "A - I-HOSP\n",
      "▁EMERGENCY - O\n",
      "▁DEP - O\n",
      "T - O\n",
      "▁VISIT - O\n",
      "▁VU - B-PATIENT\n",
      "ONG - I-PATIENT\n",
      ", - I-PATIENT\n",
      "XO - I-PATIENT\n",
      "CHI - I-PATIENT\n",
      "LT - I-PATIENT\n",
      "▁R - I-PATIENT\n",
      "▁515 - B-ID\n",
      "- - I-ID\n",
      "31 - I-ID\n",
      "- - I-ID\n",
      "23 - I-ID\n",
      "- - I-ID\n",
      "5 - I-ID\n",
      "▁VISIT - O\n",
      "▁DATE - O\n",
      ": - O\n",
      "▁01 - B-DATE\n",
      "/ - I-DATE\n",
      "15 - I-DATE\n",
      "/ - I-DATE\n",
      "61 - I-DATE\n",
      "▁PRESENT - O\n",
      "ING - O\n",
      "▁COMP - O\n",
      "LAIN - O\n",
      "T - O\n",
      ": - O\n",
      "▁Bleeding - O\n",
      ". - O\n",
      "▁HISTORY - O\n",
      "▁OF - O\n",
      "▁PRESENT - O\n",
      "ING - O\n",
      "▁COMP - O\n",
      "LAIN - O\n",
      "T - O\n",
      ": - O\n",
      "▁This - O\n",
      "▁71 - B-AGE\n",
      "▁year - O\n",
      "▁old - O\n",
      "▁female - O\n",
      "▁status - O\n",
      "▁post - O\n",
      "▁cardiac - O\n",
      "▁cath - O\n",
      "▁stent - O\n",
      "▁placement - O\n",
      "▁and - O\n",
      "▁then - O\n",
      "▁later - O\n",
      "▁EP - O\n",
      "▁study - O\n",
      "▁states - O\n",
      "▁that - O\n",
      "▁she - O\n",
      "▁was - O\n",
      "▁doing - O\n",
      "▁well - O\n",
      "▁until - O\n",
      "▁last - O\n",
      "▁night - O\n",
      "▁when - O\n",
      "▁she - O\n",
      "▁started - O\n",
      "▁to - O\n",
      "▁ooze - O\n",
      "▁from - O\n",
      "▁the - O\n",
      "▁groin - O\n",
      "▁site - O\n",
      ". - O\n",
      "▁She - O\n",
      "▁did - O\n",
      "▁not - O\n",
      "▁apply - O\n",
      "▁any - O\n",
      "▁pressure - O\n",
      ". - O\n",
      "▁She - O\n",
      "▁states - O\n",
      "▁that - O\n",
      "▁she - O\n",
      "▁was - O\n",
      "▁bleeding - O\n",
      "▁through - O\n",
      "▁the - O\n",
      "▁night - O\n",
      ", - O\n",
      "▁did - O\n",
      "▁not - O\n",
      "▁want - O\n",
      "▁to - O\n",
      "▁bother - O\n",
      "▁her - O\n",
      "▁brother - O\n",
      ", - O\n",
      "▁called - O\n",
      "▁Dr - O\n",
      ". - O\n",
      "▁Ap - B-STAFF\n",
      "onte - I-STAFF\n",
      "▁today - O\n",
      "▁who - O\n",
      "▁told - O\n",
      "▁her - O\n",
      "▁to - O\n",
      "▁come - O\n",
      "▁in - O\n",
      ". - O\n",
      "▁She - O\n",
      "▁states - O\n",
      "▁that - O\n",
      "▁she - O\n",
      "▁did - O\n",
      "▁not - O\n",
      "▁know - O\n",
      "▁where - O\n",
      "▁the - O\n",
      "▁site - O\n",
      "▁was - O\n",
      ". - O\n",
      "▁She - O\n",
      "▁could - O\n",
      "▁not - O\n",
      "▁see - O\n",
      "▁it - O\n",
      "▁and - O\n",
      "▁she - O\n",
      "▁did - O\n",
      "▁not - O\n",
      "▁apply - O\n",
      "▁pressure - O\n",
      ". - O\n",
      "▁She - O\n",
      "▁has - O\n",
      "▁no - O\n",
      "▁chest - O\n",
      "▁pain - O\n",
      ", - O\n",
      "▁no - O\n",
      "▁shortness - O\n",
      "▁of - O\n",
      "▁breath - O\n",
      ". - O\n",
      "▁She - O\n",
      "▁is - O\n",
      "▁not - O\n",
      "▁dizzy - O\n",
      ". - O\n",
      "▁She - O\n",
      "▁is - O\n",
      "▁on - O\n",
      "▁Love - O\n",
      "nox - O\n",
      "▁and - O\n",
      "▁Co - O\n",
      "uma - O\n",
      "din - O\n",
      "▁both - O\n",
      ". - O\n",
      "▁She - O\n",
      "▁has - O\n",
      "▁not - O\n",
      "▁had - O\n",
      "▁her - O\n",
      "▁PT - O\n",
      "▁checked - O\n",
      ". - O\n",
      "▁The - O\n",
      "▁rest - O\n",
      "▁of - O\n",
      "▁the - O\n",
      "▁systems - O\n",
      "▁were - O\n",
      "▁reviewed - O\n",
      "▁and - O\n",
      "▁are - O\n",
      "▁negative - O\n",
      ". - O\n",
      "▁PAST - O\n",
      "▁MEDICAL - O\n",
      "▁HISTORY - O\n",
      ": - O\n",
      "▁Depression - O\n",
      ", - O\n",
      "▁cardiac - O\n",
      "▁cath - O\n",
      ", - O\n",
      "▁hypothyroidism - O\n",
      ". - O\n",
      "▁MED - O\n",
      "ICATION - O\n",
      "S - O\n",
      ": - O\n",
      "▁Taken - O\n",
      "▁from - O\n",
      "▁the - O\n",
      "▁chart - O\n",
      ". - O\n",
      "▁ALL - O\n",
      "ERG - O\n",
      "IES - O\n",
      ": - O\n",
      "▁Taken - O\n",
      "▁from - O\n",
      "▁the - O\n",
      "▁chart - O\n",
      ". - O\n",
      "▁SOCIAL - O\n",
      "▁HISTORY - O\n",
      ": - O\n",
      "▁She - O\n",
      "▁lives - O\n",
      "▁with - O\n",
      "▁her - O\n",
      "▁brother - O\n",
      ". - O\n",
      "▁PHYSICAL - O\n",
      "▁EX - O\n",
      "A - O\n",
      "MINATION - O\n",
      ": - O\n",
      "▁Her - O\n",
      "▁heart - O\n",
      "▁rate - O\n",
      "▁was - O\n",
      "▁81 - O\n",
      ", - O\n",
      "▁her - O\n",
      "▁temperature - O\n",
      "▁is - O\n",
      "▁96 - O\n",
      ". - O\n",
      "5 - O\n",
      "▁degrees - O\n",
      ", - O\n",
      "▁blood - O\n",
      "▁pressure - O\n",
      "▁is - O\n",
      "▁123 - O\n",
      "/ - O\n",
      "64 - O\n",
      ". - O\n",
      "▁This - O\n",
      "▁is - O\n",
      "▁a - O\n",
      "▁well - O\n",
      "- - O\n",
      "developed - O\n",
      "▁female - O\n",
      ", - O\n",
      "▁alert - O\n",
      ", - O\n",
      "▁oriented - O\n",
      ". - O\n",
      "▁HE - O\n",
      "ENT - O\n",
      "▁exam - O\n",
      "▁is - O\n",
      "▁a - O\n",
      "traumatic - O\n",
      ", - O\n",
      "▁norm - O\n",
      "o - O\n",
      "cephalic - O\n",
      ". - O\n",
      "▁Neck - O\n",
      "▁is - O\n",
      "▁soft - O\n",
      "▁and - O\n",
      "▁supple - O\n",
      ". - O\n",
      "▁Mouth - O\n",
      "▁and - O\n",
      "▁throat - O\n",
      "▁are - O\n",
      "▁normal - O\n",
      ". - O\n",
      "▁Well - O\n",
      "- - O\n",
      "hydrate - O\n",
      "d - O\n",
      "▁moist - O\n",
      "▁mucosa - O\n",
      ". - O\n",
      "▁Con - O\n",
      "junct - O\n",
      "iva - O\n",
      "▁are - O\n",
      "▁clear - O\n",
      ". - O\n",
      "▁S - O\n",
      "cler - O\n",
      "ae - O\n",
      "▁are - O\n",
      "▁non - O\n",
      "- - O\n",
      "ic - O\n",
      "teric - O\n",
      ". - O\n",
      "▁There - O\n",
      "▁is - O\n",
      "▁no - O\n",
      "▁jugular - O\n",
      "▁venous - O\n",
      "▁dis - O\n",
      "tention - O\n",
      ". - O\n",
      "▁Heart - O\n",
      "▁had - O\n",
      "▁a - O\n",
      "▁regular - O\n",
      "▁rate - O\n",
      "▁and - O\n",
      "▁rhythm - O\n",
      ". - O\n",
      "▁The - O\n",
      "▁lungs - O\n",
      "▁were - O\n",
      "▁clear - O\n",
      "▁to - O\n",
      "▁aus - O\n",
      "cult - O\n",
      "ation - O\n",
      "▁bilaterally - O\n",
      ". - O\n",
      "▁The - O\n",
      "▁abdomen - O\n",
      "▁was - O\n",
      "▁soft - O\n",
      ", - O\n",
      "▁non - O\n",
      "tender - O\n",
      ". - O\n",
      "▁No - O\n",
      "▁guarding - O\n",
      ". - O\n",
      "▁No - O\n",
      "▁hepato - O\n",
      "splen - O\n",
      "omega - O\n",
      "ly - O\n",
      ". - O\n",
      "▁There - O\n",
      "▁is - O\n",
      "▁a - O\n",
      "▁hematoma - O\n",
      "▁at - O\n",
      "▁the - O\n",
      "▁right - O\n",
      "▁groin - O\n",
      ". - O\n",
      "▁The - O\n",
      "▁cath - O\n",
      "▁site - O\n",
      "▁was - O\n",
      "▁about - O\n",
      "▁1 - O\n",
      "▁cm - O\n",
      "▁and - O\n",
      "▁closed - O\n",
      "▁but - O\n",
      "▁there - O\n",
      "▁was - O\n",
      "▁some - O\n",
      "▁small - O\n",
      "▁oozing - O\n",
      "▁from - O\n",
      "▁the - O\n",
      "▁site - O\n",
      ". - O\n",
      "▁It - O\n",
      "▁stopped - O\n",
      "▁with - O\n",
      "▁pressure - O\n",
      ". - O\n",
      "▁The - O\n",
      "▁patient - O\n",
      "▁was - O\n",
      "▁covered - O\n",
      "▁in - O\n",
      "▁blood - O\n",
      "▁and - O\n",
      "▁blood - O\n",
      "▁clots - O\n",
      ". - O\n",
      "▁The - O\n",
      "▁old - O\n",
      "▁dressing - O\n",
      "▁was - O\n",
      "▁removed - O\n",
      ". - O\n",
      "▁All - O\n",
      "▁the - O\n",
      "▁dried - O\n",
      "▁blood - O\n",
      "▁and - O\n",
      "▁blood - O\n",
      "▁clots - O\n",
      "▁were - O\n",
      "▁removed - O\n",
      ". - O\n",
      "▁The - O\n",
      "▁patient - O\n",
      "▁was - O\n",
      "▁cleaned - O\n",
      "▁up - O\n",
      "▁by - O\n",
      "▁myself - O\n",
      ". - O\n",
      "▁Then - O\n",
      ", - O\n",
      "▁the - O\n",
      "▁nurses - O\n",
      "▁put - O\n",
      "▁her - O\n",
      "▁in - O\n",
      "▁the - O\n",
      "▁hospital - O\n",
      "▁gown - O\n",
      ". - O\n",
      "▁LABOR - O\n",
      "A - O\n",
      "TORY - O\n",
      "▁EVA - O\n",
      "LU - O\n",
      "ATION - O\n",
      ": - O\n",
      "▁PT - O\n",
      ", - O\n",
      "▁PTT - O\n",
      ", - O\n",
      "▁CBC - O\n",
      "▁were - O\n",
      "▁obtained - O\n",
      ". - O\n",
      "▁THE - O\n",
      "RAP - O\n",
      "Y - O\n",
      "▁R - O\n",
      "ENDER - O\n",
      "ED - O\n",
      "/ - O\n",
      "C - O\n",
      "OUR - O\n",
      "SE - O\n",
      "▁IN - O\n",
      "▁ED - O\n",
      ": - O\n",
      "▁After - O\n",
      "▁speaking - O\n",
      "▁with - O\n",
      "▁the - O\n",
      "▁patient - O\n",
      "▁at - O\n",
      "▁length - O\n",
      ", - O\n",
      "▁it - O\n",
      "▁seems - O\n",
      "▁that - O\n",
      "▁she - O\n",
      "▁lives - O\n",
      "▁with - O\n",
      "▁her - O\n",
      "▁older - O\n",
      "▁brother - O\n",
      ". - O\n",
      "▁She - O\n",
      "▁is - O\n",
      "▁caring - O\n",
      "▁for - O\n",
      "▁him - O\n",
      "▁and - O\n",
      "▁is - O\n",
      "▁a - O\n",
      "▁bit - O\n",
      "▁overwhelmed - O\n",
      "▁with - O\n",
      "▁this - O\n",
      ". - O\n",
      "▁Obviously - O\n",
      ", - O\n",
      "▁her - O\n",
      "▁medical - O\n",
      "▁knowledge - O\n",
      "▁base - O\n",
      "▁is - O\n",
      "▁not - O\n",
      "▁adequately - O\n",
      "▁prepared - O\n",
      "▁to - O\n",
      "▁deal - O\n",
      "▁with - O\n",
      "▁this - O\n",
      "▁problem - O\n",
      ". - O\n",
      "▁She - O\n",
      "▁will - O\n",
      "▁be - O\n",
      "▁admitted - O\n",
      "▁to - O\n",
      "▁the - O\n",
      "▁Observation - O\n",
      "▁Area - O\n",
      "▁through - O\n",
      "▁the - O\n",
      "▁day - O\n",
      "▁for - O\n",
      "▁nurse - O\n",
      "▁training - O\n",
      ". - O\n",
      "▁Social - O\n",
      "▁Services - O\n",
      "▁will - O\n",
      "▁be - O\n",
      "▁consulted - O\n",
      "▁to - O\n",
      "▁arrange - O\n",
      "▁for - O\n",
      "▁a - O\n",
      "▁home - O\n",
      "▁visiting - O\n",
      "▁nurse - O\n",
      ". - O\n",
      "▁Dr - O\n",
      ". - O\n",
      "▁Ap - B-STAFF\n",
      "onte - I-STAFF\n",
      "▁was - O\n",
      "▁called - O\n",
      ". - O\n",
      "▁CONSULT - O\n",
      "ATIONS - O\n",
      "▁( - O\n",
      "including - O\n",
      "▁PCP - O\n",
      ") - O\n",
      ": - O\n",
      "▁Social - O\n",
      "▁Services - O\n",
      "▁and - O\n",
      "▁Dr - O\n",
      ". - O\n",
      "▁Ap - B-STAFF\n",
      "onte - I-STAFF\n",
      ". - O\n",
      "▁FINAL - O\n",
      "▁DIA - O\n",
      "GN - O\n",
      "OSIS - O\n",
      ": - O\n",
      "▁Bleeding - O\n",
      "▁from - O\n",
      "▁cath - O\n",
      "▁site - O\n",
      "▁and - O\n",
      "▁knowledge - O\n",
      "▁deficit - O\n",
      ". - O\n",
      "▁DIS - O\n",
      "POSITION - O\n",
      "▁( - O\n",
      "including - O\n",
      "▁condition - O\n",
      "▁upon - O\n",
      "▁discharge - O\n",
      ") - O\n",
      ": - O\n",
      "▁See - O\n",
      "▁the - O\n",
      "▁chart - O\n",
      "▁for - O\n",
      "▁disposition - O\n",
      ". - O\n",
      "▁_ - O\n",
      "_ - O\n",
      "_ - O\n",
      "_ - O\n",
      "_ - O\n",
      "_ - O\n",
      "_ - O\n",
      "_ - O\n",
      "_ - O\n",
      "_ - O\n",
      "_ - O\n",
      "_ - O\n",
      "_ - O\n",
      "_ - O\n",
      "_ - O\n",
      "_ - O\n",
      "_ - O\n",
      "_ - O\n",
      "_ - O\n",
      "_ - O\n",
      "_ - O\n",
      "_ - O\n",
      "_ - O\n",
      "_ - O\n",
      "_ - O\n",
      "_ - O\n",
      "_ - O\n",
      "_ - O\n",
      "_ - O\n",
      "_ - O\n",
      "_ - O\n",
      "_ - O\n",
      "_ - O\n",
      "_ - O\n",
      "_ - O\n",
      "▁OO - B-ID\n",
      "412 - I-ID\n",
      "/ - I-ID\n",
      "09 - I-ID\n",
      "605 - I-ID\n",
      "▁NAT - B-STAFF\n",
      "HAN - I-STAFF\n",
      "▁PLAT - I-STAFF\n",
      "T - I-STAFF\n",
      ", - O\n",
      "▁M - O\n",
      ". - O\n",
      "D - O\n",
      ". - O\n",
      "▁NP - B-STAFF\n",
      "27 - I-STAFF\n",
      "▁D - O\n",
      ": - O\n",
      "01 - B-DATE\n",
      "/ - I-DATE\n",
      "15 - I-DATE\n",
      "/ - I-DATE\n",
      "61 - I-DATE\n",
      "▁T - O\n",
      ": - O\n",
      "01 - B-DATE\n",
      "/ - I-DATE\n",
      "15 - I-DATE\n",
      "/ - I-DATE\n",
      "61 - I-DATE\n",
      "▁Dict - O\n",
      "ated - O\n",
      "▁by - O\n",
      ": - O\n",
      "▁NAT - B-STAFF\n",
      "HAN - I-STAFF\n",
      "▁PLAT - I-STAFF\n",
      "T - I-STAFF\n",
      ", - O\n",
      "▁M - O\n",
      ". - O\n",
      "D - O\n",
      ". - O\n",
      "▁NP - B-STAFF\n",
      "27 - I-STAFF\n",
      "▁cc - O\n",
      ": - O\n",
      "▁O - B-STAFF\n",
      "WEN - I-STAFF\n",
      "▁R - I-STAFF\n",
      ". - I-STAFF\n",
      "▁A - I-STAFF\n",
      "PON - I-STAFF\n",
      "TE - I-STAFF\n",
      ", - O\n",
      "▁M - O\n",
      ". - O\n",
      "D - O\n",
      ". - O\n",
      "▁OA - B-STAFF\n",
      "0 - I-STAFF\n",
      "▁* - O\n",
      "* - O\n",
      "* - O\n",
      "* - O\n",
      "* - O\n",
      "* - O\n",
      "* - O\n",
      "* - O\n",
      "▁Not - O\n",
      "▁reviewed - O\n",
      "▁by - O\n",
      "▁Attending - O\n",
      "▁Physician - O\n",
      "▁* - O\n",
      "* - O\n",
      "* - O\n",
      "* - O\n",
      "* - O\n",
      "* - O\n",
      "* - O\n",
      "* - O\n",
      "[SEP] - O\n"
     ]
    }
   ],
   "source": [
    "# Print out mapping of the tokens to the labels\n",
    "tokens = tokz.convert_ids_to_tokens(tok_ds_processed['input_ids'][1])\n",
    "labels = [list(label_map.keys())[list(label_map.values()).index(label_id)] for label_id in tok_ds_processed['labels'][1]]\n",
    "\n",
    "for token, label in zip(tokens, labels):\n",
    "    print(f'{token} - {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86107c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'spans', 'meta', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 592\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'spans', 'meta', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 198\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create test and validation sets\n",
    "## test_tok_ds_processed is the test set\n",
    "tok_dds = tok_ds_processed.train_test_split(test_size=0.25, seed=42)\n",
    "tok_dds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b9f76fa",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "897d822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "268b76e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 128\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9bbc88a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 8e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "acd09f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=False,\n",
    "    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
    "    num_train_epochs=epochs, weight_decay=0.01, report_to='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2aeb96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88bd1623",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(model_nm, num_labels=len(label_map))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f259649b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForTokenClassification: ['mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_nm, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a219f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, args, train_dataset=tok_dds['train'], eval_dataset=tok_dds['test'], tokenizer=tokz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a681a4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manibatra/mambaforge/envs/phi/lib/python3.11/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge/envs/phi/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:718\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 718\u001b[0m     tensor \u001b[39m=\u001b[39m as_tensor(value)\n\u001b[1;32m    720\u001b[0m     \u001b[39m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[39m# # at-least2d\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[39m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     \u001b[39m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     \u001b[39m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    725\u001b[0m     \u001b[39m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 600 at dim 1 (got 448)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/mambaforge/envs/phi/lib/python3.11/site-packages/transformers/trainer.py:1664\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1661\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1662\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1663\u001b[0m )\n\u001b[0;32m-> 1664\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1665\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1666\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1667\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1668\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1669\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/envs/phi/lib/python3.11/site-packages/transformers/trainer.py:1909\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1906\u001b[0m     rng_to_sync \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1908\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m-> 1909\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1910\u001b[0m     total_batched_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1911\u001b[0m     \u001b[39mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[0;32m~/mambaforge/envs/phi/lib/python3.11/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/mambaforge/envs/phi/lib/python3.11/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/mambaforge/envs/phi/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/mambaforge/envs/phi/lib/python3.11/site-packages/transformers/data/data_collator.py:249\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, features: List[Dict[\u001b[39mstr\u001b[39m, Any]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[0;32m--> 249\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad(\n\u001b[1;32m    250\u001b[0m         features,\n\u001b[1;32m    251\u001b[0m         padding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding,\n\u001b[1;32m    252\u001b[0m         max_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_length,\n\u001b[1;32m    253\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    254\u001b[0m         return_tensors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreturn_tensors,\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m batch:\n\u001b[1;32m    257\u001b[0m         batch[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/mambaforge/envs/phi/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3045\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3042\u001b[0m             batch_outputs[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m   3043\u001b[0m         batch_outputs[key]\u001b[39m.\u001b[39mappend(value)\n\u001b[0;32m-> 3045\u001b[0m \u001b[39mreturn\u001b[39;00m BatchEncoding(batch_outputs, tensor_type\u001b[39m=\u001b[39;49mreturn_tensors)\n",
      "File \u001b[0;32m~/mambaforge/envs/phi/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:211\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    207\u001b[0m     n_sequences \u001b[39m=\u001b[39m encoding[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mn_sequences\n\u001b[1;32m    209\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_sequences \u001b[39m=\u001b[39m n_sequences\n\u001b[0;32m--> 211\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_to_tensors(tensor_type\u001b[39m=\u001b[39;49mtensor_type, prepend_batch_axis\u001b[39m=\u001b[39;49mprepend_batch_axis)\n",
      "File \u001b[0;32m~/mambaforge/envs/phi/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:734\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    729\u001b[0m         \u001b[39mif\u001b[39;00m key \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39moverflowing_tokens\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    730\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    731\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    732\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    733\u001b[0m             ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m--> 734\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    735\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    736\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpadding=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtruncation=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    737\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m features (`\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    738\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m expected).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    739\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    741\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f14998a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tok_dds['test']['labels'][0]\n",
    "tok_dds['train']['attention_mask'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b1b84feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1750\n",
      "1242\n",
      "1926\n",
      "1207\n",
      "693\n",
      "1429\n",
      "1164\n",
      "300\n",
      "2554\n",
      "2549\n",
      "567\n",
      "919\n",
      "391\n",
      "2314\n",
      "410\n",
      "663\n",
      "821\n",
      "449\n",
      "1110\n",
      "667\n",
      "697\n",
      "552\n",
      "639\n",
      "646\n",
      "931\n",
      "820\n",
      "665\n",
      "538\n",
      "775\n",
      "990\n",
      "1012\n",
      "2312\n",
      "157\n",
      "656\n",
      "902\n",
      "1790\n",
      "548\n",
      "1888\n",
      "462\n",
      "646\n",
      "334\n",
      "790\n",
      "1786\n",
      "1115\n",
      "1246\n",
      "698\n",
      "930\n",
      "1158\n",
      "778\n",
      "782\n",
      "492\n",
      "681\n",
      "1591\n",
      "723\n",
      "1335\n",
      "1593\n",
      "2339\n",
      "1400\n",
      "465\n",
      "566\n",
      "602\n",
      "1505\n",
      "597\n",
      "872\n",
      "2058\n",
      "1955\n",
      "1291\n",
      "865\n",
      "694\n",
      "1049\n",
      "716\n",
      "2191\n",
      "541\n",
      "1416\n",
      "1066\n",
      "1040\n",
      "246\n",
      "546\n",
      "726\n",
      "819\n",
      "743\n",
      "528\n",
      "1193\n",
      "867\n",
      "1040\n",
      "1762\n",
      "696\n",
      "961\n",
      "601\n",
      "481\n",
      "431\n",
      "1537\n",
      "213\n",
      "666\n",
      "378\n",
      "1346\n",
      "1479\n",
      "432\n",
      "613\n",
      "589\n",
      "1953\n",
      "3064\n",
      "639\n",
      "514\n",
      "347\n",
      "1816\n",
      "306\n",
      "788\n",
      "361\n",
      "566\n",
      "1415\n",
      "549\n",
      "538\n",
      "789\n",
      "3941\n",
      "492\n",
      "735\n",
      "239\n",
      "1231\n",
      "1912\n",
      "1504\n",
      "635\n",
      "748\n",
      "1023\n",
      "3569\n",
      "866\n",
      "659\n",
      "394\n",
      "534\n",
      "923\n",
      "1718\n",
      "1072\n",
      "590\n",
      "733\n",
      "1514\n",
      "942\n",
      "742\n",
      "756\n",
      "550\n",
      "1204\n",
      "403\n",
      "802\n",
      "344\n",
      "895\n",
      "2251\n",
      "2171\n",
      "2353\n",
      "362\n",
      "1951\n",
      "460\n",
      "556\n",
      "885\n",
      "1293\n",
      "584\n",
      "1539\n",
      "1385\n",
      "1406\n",
      "353\n",
      "1543\n",
      "704\n",
      "1392\n",
      "1495\n",
      "1101\n",
      "2619\n",
      "1114\n",
      "1002\n",
      "950\n",
      "932\n",
      "1234\n",
      "743\n",
      "1384\n",
      "1118\n",
      "1125\n",
      "675\n",
      "906\n",
      "1091\n",
      "1124\n",
      "400\n",
      "567\n",
      "628\n",
      "659\n",
      "1175\n",
      "1577\n",
      "603\n",
      "617\n",
      "831\n",
      "1914\n",
      "713\n",
      "1033\n",
      "418\n",
      "630\n",
      "740\n",
      "607\n",
      "477\n",
      "658\n",
      "1474\n",
      "420\n",
      "1545\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(tok_dds['test']['input_ids'])):\n",
    "    print(len(tok_dds['test']['input_ids'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825c338d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phi",
   "language": "python",
   "name": "phi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
