{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef590e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5591e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_1 = './i2b2_parsing/parsed_gold_set_1.jsonl'\n",
    "test_path = './i2b2_parsing/parsed_test.jsonl'\n",
    "data = pd.read_json(file_path_1, lines=True)\n",
    "test_data = pd.read_json(test_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25780753",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fef6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data['text'].tolist() # type: ignore\n",
    "test_texts = test_data['text'].tolist() # type: ignore\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d14d702",
   "metadata": {},
   "outputs": [],
   "source": [
    "spans = data['spans'].tolist() # type: ignore\n",
    "test_spans = test_data['spans'].tolist() # type: ignore\n",
    "spans[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cbf1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nm = 'microsoft/deberta-v3-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87e6e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, convert_slow_tokenizer\n",
    "tokz = AutoTokenizer.from_pretrained(model_nm, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d73d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_function(x): return tokz(x['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d594c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "ds = Dataset.from_pandas(data)\n",
    "test_ds = Dataset.from_pandas(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf4b285",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_ds = ds.map(tok_function, batched=True)\n",
    "test_tok_ds = test_ds.map(tok_function, batched=True)\n",
    "tok_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913a464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_ds['text'][0]\n",
    "tok_ds['input_ids'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656e01fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['spans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278a0980",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tok_ds['input_ids'][0]\n",
    "tokens = tokz.convert_ids_to_tokens(input_ids)\n",
    "token_positions = tokz(tok_ds['text'][0], return_offsets_mapping=True)['offset_mapping']\n",
    "\n",
    "for token, input_id, token_pos in zip(tokens, input_ids, token_positions):\n",
    "    print(f'{token}: {input_id}: {token_pos}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58251d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    \"Padding\": -100,\n",
    "    \"O\": 0,\n",
    "    \"B-DATE\": 1,\n",
    "    \"I-DATE\": 2,\n",
    "    \"B-PATIENT\": 3,\n",
    "    \"I-PATIENT\": 4,\n",
    "    \"B-AGE\": 5,\n",
    "    \"I-AGE\": 6,\n",
    "    \"B-STAFF\": 7,\n",
    "    \"I-STAFF\": 8,\n",
    "    \"B-PHONE\": 9,\n",
    "    \"I-PHONE\": 10,\n",
    "    \"B-EMAIL\": 11,\n",
    "    \"I-EMAIL\": 12,\n",
    "    \"B-ID\": 13,\n",
    "    \"I-ID\": 14,\n",
    "    \"B-HOSP\": 15,\n",
    "    \"I-HOSP\": 16,\n",
    "    \"B-PATORG\": 17,\n",
    "    \"I-PATORG\": 18,\n",
    "    \"B-LOC\": 19,\n",
    "    \"I-LOC\": 20,\n",
    "    \"B-OTHERPHI\": 21,\n",
    "    \"I-OTHERPHI\": 22,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4522ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_data(text, spans, input_ids):\n",
    "    tokens = tokz.convert_ids_to_tokens(input_ids)\n",
    "    labels = ['O'] * len(input_ids)\n",
    "    token_positions = tokz(text, return_offsets_mapping=True)['offset_mapping']\n",
    "\n",
    "\n",
    "    for span in spans:\n",
    "        start, end, label = span['start'], span['end'], span['label']\n",
    "\n",
    "        token_start, token_end = None, None\n",
    "\n",
    "        for idx, (char_start, char_end) in enumerate(token_positions):\n",
    "            if tokens[idx].startswith('â–'):\n",
    "                char_start += 1\n",
    "            # print(tokens[idx], char_start, char_end, start, end)\n",
    "            if char_start == int(start):\n",
    "                token_start = idx\n",
    "            if char_end == int(end):\n",
    "                token_end = idx\n",
    "                break\n",
    "        \n",
    "        if token_start is not None and token_end is not None:\n",
    "            # print(token_start, token_end, label)\n",
    "            labels[token_start] = f'B-{label}'\n",
    "            for idx in range(token_start + 1, token_end + 1):\n",
    "                labels[idx] = f'I-{label}'\n",
    "\n",
    "    # input_ids = tokz.convert_tokens_to_ids(tokens)\n",
    "    label_ids = [label_map[label] for label in labels]\n",
    "\n",
    "    return label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988c19f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_ds_processed = tok_ds.add_column('labels', [pre_process_data(text, spans, input_ids) for text, spans, input_ids in zip(tok_ds['text'], tok_ds['spans'], tok_ds['input_ids'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "615b5593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4995"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_max_len(x):\n",
    "    return max([len(input_ids) for input_ids in x['input_ids']])\n",
    "\n",
    "max_len = find_max_len(tok_ds_processed)\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d520990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_data(sequences, max_length, padding_value):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) > max_length:\n",
    "            padded_seq = seq[:max_length]\n",
    "        else:\n",
    "            padded_seq = seq + [padding_value] * (max_length - len(seq))\n",
    "        padded_sequences.append(padded_seq)\n",
    "    return padded_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81798b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = pad_data(tok_ds_processed['input_ids'], max_len, 0)\n",
    "labels = pad_data(tok_ds_processed['labels'], max_len, -100)\n",
    "attention_mask = pad_data(tok_ds_processed['attention_mask'], max_len, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98fa6a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_ds_processed = tok_ds_processed.remove_columns(['input_ids', 'labels', 'attention_mask', 'text', 'spans', 'meta'])\n",
    "tok_ds_processed = tok_ds_processed.add_column('input_ids', input_ids)\n",
    "tok_ds_processed = tok_ds_processed.add_column('labels', labels)\n",
    "tok_ds_processed = tok_ds_processed.add_column('attention_mask', attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c364441",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tok_ds_processed = test_tok_ds.add_column('labels', [pre_process_data(text, spans, input_ids) for text, spans, input_ids in zip(test_tok_ds['text'], test_tok_ds['spans'], test_tok_ds['input_ids'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cf791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out mapping of the tokens to the labels\n",
    "tokens = tokz.convert_ids_to_tokens(tok_ds_processed['input_ids'][1])\n",
    "labels = [list(label_map.keys())[list(label_map.values()).index(label_id)] for label_id in tok_ds_processed['labels'][1]]\n",
    "\n",
    "for token, label in zip(tokens, labels):\n",
    "    print(f'{token} - {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86107c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['token_type_ids', 'input_ids', 'labels', 'attention_mask'],\n",
       "        num_rows: 592\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['token_type_ids', 'input_ids', 'labels', 'attention_mask'],\n",
       "        num_rows: 198\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create test and validation sets\n",
    "## test_tok_ds_processed is the test set\n",
    "tok_dds = tok_ds_processed.train_test_split(test_size=0.25, seed=42)\n",
    "tok_dds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b9f76fa",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "268b76e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9bbc88a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 8e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "acd09f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n",
    "    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
    "    num_train_epochs=epochs, weight_decay=0.01, report_to='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2aeb96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88bd1623",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(model_nm, num_labels=len(label_map))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef43ddc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/manibatra/code/experiments/ai/phi/ehr_deidentification/phi_deidentification.ipynb Cell 30\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/manibatra/code/experiments/ai/phi/ehr_deidentification/phi_deidentification.ipynb#X52sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m metric \u001b[39m=\u001b[39m evaluate\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/manibatra/code/experiments/ai/phi/ehr_deidentification/phi_deidentification.ipynb#X52sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_metrics\u001b[39m(eval_pred):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/manibatra/code/experiments/ai/phi/ehr_deidentification/phi_deidentification.ipynb#X52sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     logits, labels \u001b[39m=\u001b[39m eval_pred\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate' is not defined"
     ]
    }
   ],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f259649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_nm, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a219f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, args, train_dataset=tok_dds['train'], eval_dataset=tok_dds['test'], tokenizer=tokz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a681a4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f14998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tok_dds['test']['labels'][0]\n",
    "tok_dds['train']['attention_mask'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b84feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tok_dds['test']['input_ids'])):\n",
    "    print(len(tok_dds['test']['input_ids'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825c338d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
